from fastapi import FastAPI
import json
import openai
import requests
from bs4 import BeautifulSoup
from fuzzywuzzy import process  # âœ… å¼•å…¥æ¨¡ç³ŠåŒ¹é…åº“
import re  # âœ… ç”¨äºåˆ†è¯
import time

app = FastAPI()

# è½½å…¥ FAQ æ•°æ®
with open("faq.json", "r", encoding="utf-8") as f:
    faq_data = json.load(f)["faq"]

# OpenAI API Key
openai.api_key = ""

# éœ€è¦çˆ¬å–çš„å¤šä¸ªå­¦æ ¡ç½‘ç«™
SCHOOL_WEBSITES = [
    "https://www.jcu.edu.sg/courses-and-study/orientation",
    "https://www.jcu.edu.sg/courses-and-study/orientation/before-orientation/",
    "https://www.jcu.edu.sg/courses-and-study/orientation/during-orientation",
    "https://www.jcu.edu.sg/courses-and-study/orientation/after-orientation",
    "https://www.jcu.edu.sg/events",
    "https://www.jcu.edu.sg/current-students/campus-maps-And-information",
]


def fetch_website_text(url):
    """çˆ¬å–å•ä¸ªç½‘ç«™çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            return f"âŒ æ— æ³•è®¿é—® {url}ï¼ŒçŠ¶æ€ç : {response.status_code}"

        soup = BeautifulSoup(response.text, "html.parser")

        # **åˆ é™¤æ— å…³çš„ HTML æ ‡ç­¾ï¼Œé¿å…çˆ¬åˆ°èœå•ã€é¡µè„š**
        for tag in soup(["script", "style", "nav", "footer", "aside"]):
            tag.extract()

        # **å°è¯•è·å–ä¸»è¦å†…å®¹**
        main_content = soup.find("main") or soup.find("article") or soup.find("section")
        if main_content:
            text_content = main_content.get_text(separator="\n", strip=True)
        else:
            text_content = soup.get_text(separator="\n", strip=True)

        return f"ğŸ”¹ æ¥æº: {url}\n{text_content}"

    except requests.RequestException as e:
        return f"âŒ è®¿é—® {url} å¤±è´¥: {str(e)}"


def fetch_multiple_websites():
    """çˆ¬å–å¤šä¸ªå­¦æ ¡ç½‘ç«™çš„å®Œæ•´æ–‡æœ¬"""
    all_content = ""

    for url in SCHOOL_WEBSITES:
        all_content += fetch_website_text(url) + "\n\n"

    return all_content if all_content else "âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•ç½‘ç«™å†…å®¹ã€‚"


def get_faq_response(user_query):
    """ä¼˜åŒ–ç‰ˆ FAQ å…³é”®è¯åŒ¹é… + æ¨¡ç³ŠåŒ¹é…"""
    for item in faq_data:
        if user_query.lower() == item["question"].lower():
            return item["answer"]

    user_keywords = set(re.findall(r'\w+', user_query.lower()))
    best_match = None
    best_score = 0

    for item in faq_data:
        faq_keywords = set(re.findall(r'\w+', item["question"].lower()))
        common_keywords = user_keywords & faq_keywords
        union_keywords = user_keywords | faq_keywords  # æ±‚å¹¶é›†
        score = len(common_keywords) / len(union_keywords)  # Jaccard ç›¸ä¼¼åº¦

        if score > best_score:
            best_score = score
            best_match = item

    if best_match and best_score > 0.4:
        return best_match["answer"]

    # æ¨¡ç³ŠåŒ¹é…
    faq_questions = [item["question"] for item in faq_data]
    best_match_fuzzy = process.extractOne(user_query, faq_questions)

    if best_match_fuzzy and best_match_fuzzy[1] > 75:
        for item in faq_data:
            if item["question"] == best_match_fuzzy[0]:
                return item["answer"]

    return None


def extract_relevant_content(user_query, content):
    """ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æå–ç½‘é¡µå†…å®¹ï¼ŒåŒæ—¶ä¼˜å…ˆæŸ¥æ‰¾åˆ—è¡¨å’Œè¡¨æ ¼"""
    paragraphs = content.split("\n")
    best_matches = process.extract(user_query, paragraphs, limit=5)

    # æå–åˆ—è¡¨å†…å®¹
    lists = "\n".join([p[0] for p in best_matches if any(symbol in p[0] for symbol in ["â€¢", "âœ”", "-", "*"])])
    normal_text = "\n".join([p[0] for p in best_matches if p[1] > 60])

    return lists + "\n" + normal_text


@app.get("/chatbot/")
def chatbot_response(query: str):
    """è·å–ç”¨æˆ·æé—®çš„å›ç­”"""
    try:
        # **1ï¸âƒ£ å…ˆå°è¯•åŒ¹é… FAQ**
        response = get_faq_response(query)
        if response:
            return {"response": response}

        # **2ï¸âƒ£ è·å–å­¦æ ¡å®˜ç½‘çš„æœ€æ–°å†…å®¹ï¼ˆæ¯æ¬¡éƒ½é‡æ–°çˆ¬å–ï¼‰**
        all_websites_content = fetch_multiple_websites()
        if "âŒ" in all_websites_content or len(all_websites_content) < 50:
            return {"error": "ç½‘é¡µçˆ¬å–å¤±è´¥ï¼Œæœªèƒ½è·å–æœ‰æ•ˆå†…å®¹ã€‚"}

        # **3ï¸âƒ£ åªæå–ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å†…å®¹**
        relevant_content = extract_relevant_content(query, all_websites_content)
        if not relevant_content or len(relevant_content) < 30:
            relevant_content = "No directly relevant information is available. However, based on general knowledge of JCU SG, I will provide my best answer."

        # **4ï¸âƒ£ å‘é€åˆ° OpenAI ChatGPT**
        completion = openai.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": (
                        f"Use the following multiple university websites to answer questions: {all_websites_content}\n\n"
                        "You are the AI assistant for James Cook University Singapore (JCU SG). "
                        "Your job is to provide accurate and helpful answers about JCU SG, including its courses, orientation programs, campus life, events, and other student-related topics. "
                        "Use the available university information to answer questions, but do not mention that you are extracting or analyzing data. "
                        "If the exact answer is not found, make a reasonable assumption based on JCU SG's academic and student policies."
                    ),
                },
                {"role": "user", "content": query}
            ]
        )

        gpt_response = completion.choices[0].message.content
        return {"response": gpt_response}

    except openai.AuthenticationError:
        return {"error": "Invalid OpenAI API key."}
    except openai.RateLimitError:
        return {"error": "OpenAI API quota exceeded."}
    except Exception as e:
        return {"error": f"Server Error: {str(e)}"}
